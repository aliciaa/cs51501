\label{sec:results}
The test matrix we use is defined as:
\begin{align}
	A_{ij} &=
\begin{cases}
	i, &i =j \\
0.5, &i = j\pm 1 \\
0.5, &(i, j) \in \{(1, n), (n,1)\} \\
0, &\text{otherwise}
\end{cases}\\
B_{ij} &= 
\begin{cases}
	4, &i = j \\
	-1, &i = j+1, j-1 \\
	0, &\text{otherwise}
\end{cases}
\end{align}
The convergence criteria is either the relative residual
\begin{equation}
	\dfrac{\|Ax-\lambda B x\|_2}{\|x\|_2} < \epsilon,
\end{equation}
or the absolute residual
\begin{equation}
	\|Ax-\lambda B x\|_2 < \epsilon.
\end{equation}
When $\lambda$ is very close to zero, the absolute residual works, and when $\lambda$ is not close to zero, the relative residual works. We choose $\epsilon = 10^{-4}$ in our implementation. For the matrices $A$ and $B$ with size $10000 \times 10000$. There are $39$ eigenvalues in $[0, 10]$. Their values the the residual norms computed by TraceMin algorithm are in Table~\ref{tab:eigs}. One can see they are almost evenly distributed with gap $\sim 0.25$. 

\subsection{Jacobi Methods}
As mentioned in the previous section, the 1-sided and 2-sided Jacobi methods have different pros and cons. In general,
the 1-sided Jacobi method outperforms the 2-sided Jacobi method in computing the eigen decomposition.
Figure~\ref{fig:jacobi} shows the comparisons between the two methods.

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
				ybar,
				enlargelimits=0.15,
				legend style={at={(0.5,-0.1)},
				anchor=north,legend columns=-1},
				ylabel={average time (s)},
				xtick=data,
				y tick label style={
					/pgf/number format/.cd,
					fixed,
					fixed zerofill,
					precision=2,
					/tikz/.cd
				},
				legend style={/tikz/every even column/.append style={column sep=0.5cm}}
			]
			\addplot table [x=size,y=jacobi1] {jacobi.dat};
			\addplot table [x=size,y=jacobi2] {jacobi.dat};
			\legend{1-sided Jacobi, 2-sided Jacobi}
		\end{axis}
	\end{tikzpicture}
	\caption{Time for different variations of TraceMin with parallel Jacobi for a $10,000 \times 10,000$ system}
  \label{fig:jacobi}
\end{figure}

\subsection{TraceMin}
For simple TraceMin algorithm, the result on the generated matrices $A$ and $B$ are in Table~\ref{tab:result-tracemin}. In the test we compute the smallest $20$ eigenvalues. The linear solver (CG or MinRes) takes the majority of the runtime (more than $90\%$ percent of the overall runtime). And also with more number of threads, the overall runtime do get decreased, and the major contribution comes from the linear equation solver. The speedup also gets better with larger matrices. 
\begin{table*}
\begin{center}
\begin{tabular}{| c | c | r | r | r | r | r |  r | r | r | r | r |}
\hline
$n = $ & $\#threads$ & \multicolumn{5}{|c|}{ CG} & \multicolumn{5}{|c|}{ MinRes} \\ 
\hline
& & Iter & \multicolumn{4}{|c|} {Time} & Iter & \multicolumn{4}{|c|}{Time} \\
& &       & Total & Jacobi & QR & Linear & & Total & Jacobi & QR & Linear \\
\hline
$10,000$ & 1 &12 & 40.02 & 0.44 & 0.29 & 37.98 &  12 & 16.23 & 0.44 & 0.28 & 14.29\\
$10,000$ & 2 &12 & 29.04 & 1.08 & 0.38 & 26.67 &  12 & 13.80 & 0.97 & 0.34 & 11.68\\
$10,000$ & 4 &12 & 19.54 & 0.94 & 0.42 & 17.50 &  12 & 11.91 & 0.79 & 0.34 & 10.24\\
$10,000$ & 8 &12 & 20.13 & 2.00 & 0.56 & 16.62 &  12 & 11.37 & 0.79& 0.33 & 9.77\\
\hline
$50,000$ & 1 &12 &1188.94 &0.44 &3.26 &1178.90 &  13 & 201.87 & 0.50 & 3.49 & 190.97\\
$50,000$ & 2 &12 &836.09  &0.94 &2.26 &829.11  &  13 & 117.95 & 1.07 & 2.50 & 110.29\\
$50,000$ & 4 &12 &702.03  &0.81 &2.28 &696.19  &  13 & 90.17 & 0.90 & 2.49 & 83.89\\
$50,000$ & 8 &12 &535.96  &0.85 &2.31 &530.09  &  13 & 84.14 & 0.87 & 2.45 & 78.09\\
\hline
$100,000$ & 1 &12 & & & & &  12 & 622.61 & 0.45 & 8.77 & 600.71\\
$100,000$ & 2 &12 & & & & &  12 & 406.59 & 0.97 & 6.20 & 391.31\\
$100,000$ & 4 &12 &2175.04 &0.82 &6.28 &2162.43 &  12 & 250.48 & 0.80 & 6.24 & 238.08\\
$100,000$ & 8 &12 & & & & &  12 & 236.36 & 0.79 &  6.25 & 224.16\\
\hline
\end{tabular}
\caption{Results on simple TraceMin algorithm.}
\label{tab:result-tracemin}
\end{center}
\end{table*}

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[ybar stacked,
				enlargelimits=0.15,
				legend style={at={(0.5,-0.1)},	anchor=north,legend columns=-1},
				symbolic x coords={Basic, Deflation, Davidson},
				xtick=data,
				ylabel={time (s)},
				thick,
				no markers,
				legend style={/tikz/every even column/.append style={column sep=0.5cm}}
			]
			\addplot table [x=method,y=jacobi] {comparison.dat};
			\addplot table [x=method,y=qr] {comparison.dat};
			\addplot table [x=method,y=linear] {comparison.dat};
			\addplot table [x=method,y=others] {comparison.dat};
			\legend{Jacobi, QR, Reduced System, Others}
		\end{axis}
	\end{tikzpicture}
	\caption{Breakdown of time used for different variations of TraceMin for a $50,000 \times 50,000$ system}
  \label{fig:comp}
\end{figure}

Figure~\ref{fig:comp} shows the time for different part of TraceMin in the 3 varieties of TraceMin implementations
using serial Jacobi methods. We observe that the iterative solver is the bottleneck for the basic TraceMin and
TraceMin with deflation. This is because they both search the solution in a small subspace and hence require more
iterations to converge. On the other hand, TraceMin-Davidson requires much less time in solving the reduced system
since the subspace dimension is increasing over iterations. However, when the subspace is larger, the dimension of
the matrix in the Rayleigh-Ritz procedure increases and requires more time to compute the eigen decomposition. As a
result, the eigen decomposition step becomes the bottleneck in TraceMin-Davidson.

For this test problem, the number of iterations for the basic TraceMin, TraceMin with deflation and TraceMin-Davidson
are 316, 24 and 12 respectively.

\subsection{Multisectioning}
Table~\ref{tab:result-multisection} gives the results on using multisection to compute all eigenvalues in interval
$[0, 10]$ (39 eigenvalues in total). In multisection, since we shift the origin of the problem by
$\left(A - \mu B\right) x = \lambda B x$ in which $\mu$ is the midpoint of each subsection. This will cause the
distribution of the eigenvalues to change. So even the number of eigenvalue in each subsection are the same, we may
still get different number of iterations on each subproblem. So we only report the time on the subsection which takes
the longest time to finish. But we can see that as we divide the entire interval into more subsections, the longest
runtime on those subsections are reduced. 

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[xlabel={number of nodes},
				ylabel={total time (s)},
				thick,
				no markers]
			\addplot table [x=nodes,y=time] {multisection_time.dat};
		\end{axis}
	\end{tikzpicture}
	\caption{Total time for different number of nodes used}
  \label{fig:timenodes}
\end{figure}

\begin{table*}
\begin{center}
\begin{tabular}{|c | c | c | r | r | r | r | r | r |}
\hline
$\#$proc & Intervals & $\#$eigs & MultiSection & \multicolumn{5}{|c|}{Longest Subsection} \\
  & & &  & Iter & Total & Jacobi & QR & Linear \\
\hline
1 & $[0, 10]$ & 39 & 0 & 29 & 105.36 & 16.82 & 4.76 & 74.90 \\
2 & $[0, 4.28, 10]$ & $[17, 22]$ & 0.76 &  46 & 152.10 & 2.70 & 1.34 & 143.00\\
4 & $[0, 2.67, 5.33, 7.33, 10]$ & $[10, 11, 8, 10]$ & 0.15 & 18 & 49.96 & 0.05 & 0.11 & 49.19 \\
8 &$[0, 1.29, 2.58, 3.87, $ & $[5, 5, 5, 5, 5,$ & &  & & & & \\
& $5.16, 6.45, 7.74, 8.70, 10]$& $5, 4, 5]$& 0.25 & 17 & 29.98 & 0.01 & 0.02 & 29.67\\
\hline
\end{tabular}
\caption{Multisection result on $[0, 10]$ for the $10,000 \times 10,000$ generated matrix $A$ and $B$.}
\label{tab:result-multisection}
\end{center}
\end{table*}

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
eigenvalues =  & 0.158 & 0.454 & 0.726& 0.988 &1.244 &1.497& 1.749 \\ 
residual norm =  & $2.38\times10^{-5}$ & $2.03\times10^{-5}$ & $2.16\times10{-5}$ & $2.41\times10^{-5}$ & $2.69\times10^{-5}$ & $2.43\times10^{-5}$ & $2.81\times10^{-5}$\\
\hline
\hline
eigenvalues = &1.999 & 2.250 & 2.500 & 2.750 & 3.000& 3.250& 3.500\\
residual norm =  &$2.71\times10^{-5}$ & $2.72\times10^{-5}$& $2.71\times10^{-5}$ &$2.69\times10^{-5}$ & $2.79\times10^{-5}$ &$2.85\times10^{-5}$&  $2.81\times10^{-5}$ \\
\hline
\hline
eigenvalues = &3.750 & 4.000 &4.250 & 4.500 & 4.750 & 5.000 & 5.250 \\
residual norm =  &$2.81\times10^{-5}$ & $2.79\times10^{-5}$ & $2.88\times10^{-5}$ & $3.00\times10^{-5}$ & $3.03\times10^{-5}$ & $3.13\times10^{-5}$ &$3.11\times10^{-5}$ \\
\hline
\hline
eigenvalues = &  5.500  & 5.750  & 6.000  & 6.250 & 6.500 & 6.750 & 7.000\\
residual norm = & $3.14\times10^{-5}$ &$3.34\times10^{-5}$ & $3.91\times10^{-5}$ & $4.13\times10^{-5}$ & $5.11\times10^{-5}$ & $4.99\times10^{-5}$ & $6.35\times10^{-5}$ \\
\hline
\hline
eigenvalues =  & 7.250 & 7.500 & 7.750 & 8.000 & 8.250 & 8.500 & 8.750   \\
residual norm = & $8.10\times10{-5}$ & $1.12\times10^{-4}$ & $1.26\times10^{-4}$ & $1.69\times10^{-4}$ & $2.03\times10^{-4}$ & $2.48\times10^{-4}$&  $2.99\times10^{-4}$ \\
\hline
\hline
eigenvalues =  & 9.000 & 9.250 & 9.500 & 9.750 &&&\\
residual norm = & $3.69\times10^{-4}$ &$4.50\times10^{-4}$ & $5.43\times10^{-4}$ & $6.44\times10^{-4}$&&&\\
\hline
\end{tabular}
\caption{Eigenvalues and their computed residual norms on interval $[0, 10]$ for the generated $10000 \times 10000$ matrix.}
\label{tab:eigs}
\end{center}
\end{table*}

