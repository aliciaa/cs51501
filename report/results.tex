\label{sec:results}
The test matrix we use is defined as:
\[
A_{ij} =
\begin{cases}
i, i =j \\
0.5, i = j+1, j-1 \\
0.5, (i, j) \in \{(1, n), (n,1)\} \\
0, otherwise
\end{cases}
\]
\[
B_{ij} = 
\begin{cases}
4, i = j \\
-1, i = j+1, j-1 \\
0, otherwize
\end{cases}
\]
The convergence criteria is either the relative residual $\frac{||Ax-\lambda B x||_2}{||x||_2} < \epsilon$ or the absolute residual $||Ax-\lambda B x||_2 < \epsilon$. When $\lambda$ is very close to zero, the absolute residual works, and when $\lambda$ is not close to zero, the relative residual works. We choose $\epsilon = 10^{-4}$ in our implementation. For the matrices $A$ and $B$ with size $10000 \times 10000$. There are $39$ eigenvalues in $[0, 10]$. Their values the the residual norms computed by TraceMin algorithm are in Table~\ref{tab:eigs}. One can see they are almost evenly distributed with gap $\sim 0.25$. 

\subsection{Jacobi Methods}
As mentioned in the previous section, the 1-sided and 2-sided Jacobi methods have different pros and cons. In general,
the 1-sided Jacobi method outperforms the 2-sided Jacobi method in computing the eigen decomposition.
Figure~\ref{fig:jacobi} shows the comparisons between the two methods.

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
				ybar,
				enlargelimits=0.15,
				legend style={at={(0.5,-0.1)},
				anchor=north,legend columns=-1},
				ylabel={average time (s)},
				xtick=data,
				y tick label style={
					/pgf/number format/.cd,
					fixed,
					fixed zerofill,
					precision=2,
					/tikz/.cd
				},
			]
			\addplot table [x=size,y=jacobi1] {jacobi.dat};
			\addplot table [x=size,y=jacobi2] {jacobi.dat};
			\legend{1-sided Jacobi, 2-sided Jacobi}
		\end{axis}
	\end{tikzpicture}
	\caption{Time for different variations of TraceMin with parallel Jacobi for a $10,000 \times 10,000$ system}
  \label{fig:jacobi}
\end{figure}

\subsection{TraceMin}
For simple TraceMin algorithm, the result on the generated matrices $A$ and $B$ are in Table~\ref{tab:result-tracemin}. In the test we compute the smallest $20$ eigenvalues. The linear solver (CG or MinRes) takes the majority of the runtime (more than $90\%$ percent of the overall runtime). And also with more number of threads, the overall runtime do get decreased, and the major contribution comes from the linear equation solver. The speedup also gets better with larger matrices. 
\begin{table*}
\begin{center}
\begin{tabular}{| c | c | r | r | r | r | r |  r | r | r | r | r |}
\hline
$n = $ & $\#threads$ & \multicolumn{5}{|c|}{ CG} & \multicolumn{5}{|c|}{ MinRes} \\ 
\hline
& & Iter & \multicolumn{4}{|c|} {Time} & Iter & \multicolumn{4}{|c|}{Time} \\
& &       & Total & Jacobi & QR & Linear & & Total & Jacobi & QR & Linear \\
\hline
$10,000$ & 1 &12 & 40.02 & 0.44 & 0.29 & 37.98 &  12 & 16.23 & 0.44 & 0.28 & 14.29\\
$10,000$ & 2 &12 & 29.04 & 1.08 & 0.38 & 26.67 &  12 & 13.80 & 0.97 & 0.34 & 11.68\\
$10,000$ & 4 &12 & 19.54 & 0.94 & 0.42 & 17.50 &  12 & 11.91 & 0.79 & 0.34 & 10.24\\
$10,000$ & 8 &12 & 20.13 & 2.00 & 0.56 & 16.62 &  12 & 11.37 & 0.79& 0.33 & 9.77\\
\hline
$50,000$ & 1 &12 &1188.94 &0.44 &3.26 &1178.90 &  13 & 201.87 & 0.50 & 3.49 & 190.97\\
$50,000$ & 2 &12 &836.09  &0.94 &2.26 &829.11  &  13 & 117.95 & 1.07 & 2.50 & 110.29\\
$50,000$ & 4 &12 &702.03  &0.81 &2.28 &696.19  &  13 & 90.17 & 0.90 & 2.49 & 83.89\\
$50,000$ & 8 &12 &535.96  &0.85 &2.31 &530.09  &  13 & 84.14 & 0.87 & 2.45 & 78.09\\
\hline
$100,000$ & 1 &12 & & & & &  12 & 622.61 & 0.45 & 8.77 & 600.71\\
$100,000$ & 2 &12 & & & & &  12 & 406.59 & 0.97 & 6.20 & 391.31\\
$100,000$ & 4 &12 &2175.04 &0.82 &6.28 &2162.43 &  12 & 250.48 & 0.80 & 6.24 & 238.08\\
$100,000$ & 8 &12 & & & & &  12 & 236.36 & 0.79 &  6.25 & 224.16\\
\hline
\end{tabular}
\caption{Results on simple TraceMin algorithm.}
\label{tab:result-tracemin}
\end{center}
\end{table*}

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[ybar stacked,
				enlargelimits=0.15,
				legend style={at={(0.5,-0.1)},	anchor=north,legend columns=-1},
				symbolic x coords={Basic, Deflation, Davidson},
				xtick=data,
				ylabel={total time (s)},
				thick,
				no markers]
			\addplot table [x=method,y=jacobi] {comparison.dat};
			\addplot table [x=method,y=qr] {comparison.dat};
			\addplot table [x=method,y=linear] {comparison.dat};
			\legend{Jacobi, QR, Reduced System}
		\end{axis}
	\end{tikzpicture}
	\caption{Time for different variations of TraceMin with serial Jacobi methods for a $10,000 \times 10,000$ system}
  \label{fig:comp_s}
\end{figure}

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[ybar stacked,
				enlargelimits=0.15,
				legend style={at={(0.5,-0.1)},	anchor=north,legend columns=-1},
				symbolic x coords={Basic, Deflation, Davidson},
				xtick=data,
				ylabel={total time (s)},
				thick,
				no markers]
			\addplot table [x=method,y=jacobi] {comparison_p.dat};
			\addplot table [x=method,y=qr] {comparison_p.dat};
			\addplot table [x=method,y=linear] {comparison_p.dat};
			\legend{Jacobi, QR, Reduced System}
		\end{axis}
	\end{tikzpicture}
	\caption{Time for different variations of TraceMin with parallel Jacobi for a $10,000 \times 10,000$ system}
  \label{fig:comp_p}
\end{figure}

Figure~\ref{fig:comp_s} shows the time for different part of TraceMin in the 3 varieties of TraceMin implementations
using serial Jacobi methods. We observe that the iterative solver is the bottleneck for the basic TraceMin and
TraceMin with deflation. This is because they both search the solution in a small subspace and hence require more
iterations to converge. On the other hand, TraceMin-Davidson requires much less time in solving the reduced system
since the subspace dimension is increasing over iterations. However, when the subspace is larger, the dimension of
the matrix in the Rayleigh-Ritz procedure increases and requires more time to compute the eigen decomposition. As a
result, the eigen decomposition step becomes the bottleneck in TraceMin-Davidson.

Figure~\ref{fig:comp_p} show the same system except using parallel Jacobi methods. We can observe that both the
basic TraceMin and TraceMin with deflation take more time on the eigen decomposition step. This is because the
dimension of the matrices to be decomposed are small and parallelism becomes inefficient. On the other hand, in
TraceMin-Davidson the dense matrix dimension increases over iteration, and parallelism becomes more efficient.

\subsection{Multisection}
Table~\ref{tab:result-multisection} gives the results on using multisection to compute all eigenvalues in interval $[0, 10]$ (39 eigenvalues in total). In multisection, since we shift the origin of the problem by $(A - \mu B) x = \lambda B x$ in which $\mu$ is the midpoint of each subsection. This will cause the distribution of the eigenvalues to change. So even the number of eigenvalue in each subsection are the same, we may still get different number of iterations on each subproblem. So we only report the time on the subsection which takes the longest time to finish. But we can see that as we divide the entire interval into more subsections, the longest runtime on those subsections are reduced. 

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[xlabel={number of nodes},
				ylabel={total time (s)},
				thick,
				no markers]
			\addplot table [x=nodes,y=time] {multisection_time.dat};
		\end{axis}
	\end{tikzpicture}
	\caption{Total time for different number of nodes used}
  \label{fig:timenodes}
\end{figure}

\begin{table*}
\begin{center}
\begin{tabular}{|c | c | c | r | r | r | r | r | r |}
\hline
$\#$proc & Intervals & $\#$eigs & MultiSection & \multicolumn{5}{|c|}{Longest Subsection} \\
  & & &  & Iter & Total & Jacobi & QR & Linear \\
\hline
1 & $[0, 10]$ & 39 & 0 & 29 & 105.36 & 16.82 & 4.76 & 74.90 \\
2 & $[0, 4.28, 10]$ & $[17, 22]$ & 0.76 &  46 & 152.10 & 2.70 & 1.34 & 143.00\\
4 & $[0, 2.67, 5.33, 7.33, 10]$ & $[10, 11, 8, 10]$ & 0.15 & 18 & 49.96 & 0.05 & 0.11 & 49.19 \\
8 &$[0, 1.29, 2.58, 3.87, $ & $[5, 5, 5, 5, 5,$ & &  & & & & \\
& $5.16, 6.45, 7.74, 8.70, 10]$& $5, 4, 5]$& 0.25 & 17 & 29.98 & 0.01 & 0.02 & 29.67\\
\hline
\end{tabular}
\caption{Multisection result on $[0, 10]$ for the $10000 \times 10000$ generated matrix $A$ and $B$.}
\label{tab:result-multisection}
\end{center}
\end{table*}

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
eigenvalues =  & 0.158 & 0.454 & 0.726& 0.988 &1.244 &1.497& 1.749 \\ 
residual norm =  & $2.38*10^{-5}$ & $2.03*10^{-5}$ & $2.16*10{-5}$ & $2.41*10^{-5}$ & $2.69*10^{-5}$ & $2.43*10^{-5}$ & $2.81*10^{-5}$\\
\hline
\hline
eigenvalues = &1.999 & 2.250 & 2.500 & 2.750 & 3.000& 3.250& 3.500\\
residual norm =  &$2.71*10^{-5}$ & $2.72*10^{-5}$& $2.71*10^{-5}$ &$2.69*10^{-5}$ & $2.79*10^{-5}$ &$2.85*10^{-5}$&  $2.81*10^{-5}$ \\
\hline
\hline
eigenvalues = &3.750 & 4.000 &4.250 & 4.500 & 4.750 & 5.000 & 5.250 \\
residual norm =  &$2.81*10^{-5}$ & $2.79*10^{-5}$ & $2.88*10^{-5}$ & $3.00*10^{-5}$ & $3.03*10^{-5}$ & $3.13*10^{-5}$ &$3.11*10^{-5}$ \\
\hline
\hline
eigenvalues = &  5.500  & 5.750  & 6.000  & 6.250 & 6.500 & 6.750 & 7.000\\
residual norm = & $3.14*10^{-5}$ &$3.34*10^{-5}$ & $3.91*10^{-5}$ & $4.13*10^{-5}$ & $5.11*10^{-5}$ & $4.99*10^{-5}$ & $6.35*10^{-5}$ \\
\hline
\hline
eigenvalues =  & 7.250 & 7.500 & 7.750 & 8.000 & 8.250 & 8.500 & 8.750   \\
residual norm = & $8.10*10{-5}$ & $1.12*10^{-4}$ & $1.26*10^{-4}$ & $1.69*10^{-4}$ & $2.03*10^{-4}$ & $2.48*10^{-4}$&  $2.99*10^{-4}$ \\
\hline
\hline
eigenvalues =  & 9.000 & 9.250 & 9.500 & 9.750 &&&\\
residual norm = & $3.69*10^{-4}$ &$4.50*10^{-4}$ & $5.43*10^{-4}$ & $6.44*10^{-4}$&&&\\
\hline
\end{tabular}
\caption{Eigenvalues and their computed residual norms on interval $[0, 10]$ for the generated $10000 \times 10000$ matrix.}
\label{tab:eigs}
\end{center}
\end{table*}

