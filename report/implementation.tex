\label{sec:implementation}
\subsection{Eigen Decomposition for Dense Matrices}
The $B$-orthonormalization and the Rayleigh-Ritz procedures both involve finding the eigen decomposition of a
dense matrix. In our project, we implemented the 1-sided and 2-sided Jacobi methods as the eigen decomposition
routines. They can both be parallelized in a similar manner since the plane rotations are applied on two columns
(or rows) and are independent from other columns (or rows). In the textbook\cite{gallopoulos}, it provides a
parallelism scheme to simultaneously compute $\lfloor n / 2\rfloor$ plane rotations. This scheme applies to both
1-sided and 2-sided Jacobi methods. In this project, we apply a simpilfied algorithm to compute the order of
annihilations as described in Algorithm~\ref{alg:order}.
\begin{algorithm}[!ht]
	\SetArgSty{}
	\SetKwProg{Proc}{}{ :}{}
	\KwIn{Matrix dimension $n$}
	\KwOut{The order of annihilations $\mathcal{P}_k$ for each iteration $k$ of Jacobi mehods}
	$m = \left\lfloor\dfrac{n}{2}\right\rfloor$\;
	\For{$k = 1 \rightarrow$ n}{
		$\mathcal{P}_k = \emptyset$\;
		\For{$j = \dfrac{n-k}{2} + 1 \rightarrow \dfrac{n-k}{2} + m$}{
			$i = n - k - j$\;
			\If{$i + n \ne j$}{
				\lIf{$i < 0$}{$\mathcal{P}_k = \mathcal{P}_k \cup \{(j,i+n)\}$}
				\lElse{$\mathcal{P}_k = \mathcal{P}_k \cup \{(i, j)\}$}
			}
		}
	}
	\caption{Order of Annihilations}\label{alg:order}
\end{algorithm}
Since this order of annihilation depends only on the matrix dimension, it does not change throughout the outer
loop of the trace minimization algorithms except that for the Rayleigh-Ritz procedure in TraceMin-Davidson algorithm.
Hence, the order can be computed before the outer loop and reused throughout the iterations. For Tracemin-Davidson
algorithm, new order needs to be recomputed since the dimension increases over iterations.

The two Jacobi methods differ from each other in some ways. For the 1-sided Jacobi method, it finds an orthogonal
matrix $U$ such that $A U = Q$ is a matrix of orthogonal columns and $Q$ can be written as $V \Sigma$ with
$V^T V = I$; whereas for the 2-sided Jacobi method, it finds an orthogonal matrix $U$ such that $UAU^T = D$ and
$D$ is diagonal. Plane rotations only apply on columns for 1-sided Jacobi method but apply on both columns and rows
for 2-sided Jacobi method. The angle of plane rotation depends on the 2-norms of the columns and their inner
product for 1-sided Jacobi method, and the values of the $2 \times 2$ principal submatrix for 2-sided Jacobi method.
Finally, for 1-sided Jacobi method, the absolute values of the eigenvalues are the 2-norms of the columns of $Q$ and
the eigenvectors are found by scaling the columns by the inverse of the eigenvalues. For 2-sided Jacobi method, the
resulting diagonal matrix $D$ contains the eigenvalues and the columns of matrix $U^T$ which is the product of all
the plane rotations are the eigenvectors. Since the 1-sided Jacobi method naturally computes the absolute values of
the eigenvalues, it is more suitable to be used on the $B$-orthonormalization procedure since $B$ is symmetric
positive definite and the eigenvalues of $V^T B V$ are always positive. To find negative eigenvalues by the 1-sided
Jacobi method, one needs to calculate the Rayleigh quotient $v^T A v / \|v\|_2$. During the experiments, we found
that the 2-sided Jacobi method is more numerically stable when calculating negative eigenvalues. Therefore, we
apply the 2-sided Jacobi method for the Rayleigh-Ritz procedure althogh the 1-sided Jacobi method is computationally
less expensive.

\subsection{Modified Conjugate Gradient Method}

\subsection{Minimum Residual Method}
We implemented the {\tt MinRes} linear solver as the class note described. The {\tt MinRes} linear solver works in the case when $A$ is not s.p.d, which is required in the multisection algorithm.  The flowchart of the algorithm is given in Algorithm~\ref{alg:minres}. Out implementation is a directly transcript of the describe algorithm, except that we solve each column in $N_k$ independently to get each column of $\Delta_k$. And we replace the matrix-vector multiplication $Ax$ in the {\tt Arnoldi process} into $\left(I-Q_1 Q_1^T\right)Ax$, because the system we are solving is $\left(I-Q_1 Q_1^T\right)A \Delta_k = N_k$. In each iteration, only one matrix vector multiplication is needed. And we implement the algorithm using {\tt Intel MKL}.

\begin{algorithm}[!ht]
	\SetArgSty{}
	\SetKwProg{Proc}{}{ :}{}
	\KwIn{$A$ a symmetric matrix, \newline
		 $rhs$ the right hand side vector
	}
	\KwOut{Vector $x$ is the solution of $Ax = rhs$}
	{$x_0 = 0\;$}
	{$v_1 = rhs$\;}
	{$\beta_1 = \| v_1 \|_2$\;}
	{$v_1 = v_1 / \beta_1$\;}
	\For{$k = 1 \rightarrow$ mat\_iter, or until convergence}{
	   \Proc{{\tt Arnoldi process}}{
	       $v_{k+1} = A v_{k}$\;
	       $\alpha_k = <v_k, v_{k+1}>$\;
	       $v_{k+1}  = v_{k+1} - \alpha_k v_k$\;
	       $\beta_{k+1} = \| v_{k+1}\|_2$\;
	       $v_{k+1} = v_{k+1} / \beta_{k+1}$\;
	   }
	   \Proc{Use $c_{k-2}, s_{k-2}, c_{k-1}, s_{k-1}$ to rotate $[0, \beta_{k}, \alpha_k, \beta_{k+1}]$} {
	       $[\epsilon_k, \delta_k, \dot{r}_k, \beta_{k+1}] = [0, \beta_{k}, \alpha_k, \beta_{k+1}]$ \newline $\begin{bmatrix} c_{k-2} & s_{k-2} & 0 & 0 \\ -s_{k-2} & c_{k-2} & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 0 & 0 & 0 \\ 0 & c_{k-1} & s_{k-1} & 0 \\ 0 & -s_{k-1} & c_{k-1} & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$
	   }
	   \Proc{Get $c_k, s_k$ to rotate $[\epsilon_k, \delta_k, \dot{r}_k, \beta_{k+1}]$ to $[\epsilon_k, \delta_k, r_k, 0]$ } {
	      $[\epsilon_k, \delta_k, r_k, 0] = [\epsilon_k, \delta_k, \dot{r}_k, \beta_{k+1}] \begin{bmatrix} 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 \\ 0 & 0 & c_k & s_k \\ 0 & 0 & -s_k & c_k\\ \end{bmatrix}$
	   }
	   {Solve $m_k$ from $\epsilon_k m_{k-2} + \delta_k m_{k-1} + r_k m_k = v_k$\;}
	   {$\tau_k = s_1 \cdots s_{k-1} c_k$\;}
	   {Update $x_k = x_{k-1} + \beta_1 \tau_k v_k$\;}
	   {Check convergence $r_k = A x_k - rhs$\;}
	}
	\caption{Minimum Residual Algorithm}
	\label{alg:minres}
\end{algorithm}

\subsection{Multisectioning}
When we want to compute all eigenvalues in the interval $[a, b]$. We can first perform $LDL^T$ factorization on $A-aB$ and $A-bB$ respectively. Then by counting the number of negative elements along the diagonal of $D$, we can know how many eigenvalues are smaller than $a$ and $b$ respectively in the problem $A x = \lambda B x$. Then we can know how many eigenvalues are in the interval $[a,b]$, assume it's $p$. Then we can shift the origin to $(a+b)/2$ and apply the TraceMin algorithm to solve $\left[A - (a+b)/2 B\right] x = \lambda B x$ for the first $p$ smallest absolute eigenvalues. 

To accelerate the procedure, we would like to further divide the interval $[a,b]$ into smaller subsections, and we also want each subsection to contain roughly same amount of eigenvalues. Then we can run multiple TraeMin algorithm on each subsection on different processes. So the problem is reduced to how to find the subsections. We call this problem as multisection and we developed a heuristic approach to solve the problem.

Assume we have $p$ processors, and we fix a parameter $s$ as in each round, each processor will run $s$ times of $LDL^T$ factorization. In the first round of the algorithm, the entire interval $[a, b]$ is divided in to $p \times s - 1$ subintervals, so there are $p \times s$ endpoints. Then after each processor performs $s$ times $LDL^T$ factorization, we can know the total number of eigenvalues in the entire interval $[a,b]$, the expected average $e_a$ after multisection and the number of eigenvalues in each of the $p \times s-1$ subinterval.

Then we check for all subintervals, if the number of eigenvalue in that interval is greater than $e_a$ times a tolerance, $10\%$ in our implementation. If it does, we further divide this subinterval in to $p \times s+1$ subsubintervals, and let each processor run $s$ times $LDL^T$ factorizations to calculate the number of eigenvalues on each subsubintervals. We repeat this process until the number of eigenvalue in all finest intervals are smaller than $e_a \times 10\%$. 

After that we begin with the left most endpoint $a$, keep combining subintervals until the number of eigenvalues are close to the expected average $e_a$. Then we say this is a subsection in the multisection. We keep this procedure until the entire interval $[a,b]$ is divide into $p$ subsections. And we also know the number of eigenvalues in each subsection. Then we can run $p$ TraceMin algorithms on $p$ nodes in parallel to evaluate all eigenvalues in interval $[a,b]$.

In the implementation, we set $s = 4$ and tolerance as $10\%$. The rational behind the tolerance is that we can guarantee that the difference between the number of eigenvalues in each subsection and the expected average will be no more than $ea \times 5\%$. And we use {\tt PARDISO} to compute the $LDL^T$ factorization.



