\label{sec:implementation}
\subsection{Eigen Decomposition for Dense Matrices}
The $B$-orthonormalization and the Rayleigh-Ritz procedures both involve finding the eigen decomposition of a
dense matrix. In our project, we implemented the 1-sided and 2-sided Jacobi methods as the eigen decomposition
routines. They can both be parallelized in a similar manner since the plane rotations are applied on two columns
(or rows) and are independent from other columns (or rows). In the textbook\cite{gallopoulos}, it provides a
parallelism scheme to simultaneously compute $\lfloor n / 2\rfloor$ plane rotations. This scheme applies to both
1-sided and 2-sided Jacobi methods. In this project, we apply a simpilfied algorithm to compute the order of
annihilations as described in Algorithm~\ref{alg:order}.
\begin{algorithm}[!ht]
	\SetArgSty{}
	\SetKwProg{Proc}{}{ :}{}
	\KwIn{Matrix dimension $n$}
	\KwOut{The order of annihilations $\mathcal{P}_k$ for each iteration $k$ of Jacobi mehods}
	$m = \left\lfloor\dfrac{n}{2}\right\rfloor$\;
	\For{$k = 1 \rightarrow$ n}{
		$\mathcal{P}_k = \emptyset$\;
		\For{$j = \dfrac{n-k}{2} + 1 \rightarrow \dfrac{n-k}{2} + m$}{
			$i = n - k - j$\;
			\If{$i + n \ne j$}{
				\lIf{$i < 0$}{$\mathcal{P}_k = \mathcal{P}_k \cup \{(j,i+n)\}$}
				\lElse{$\mathcal{P}_k = \mathcal{P}_k \cup \{(i, j)\}$}
			}
		}
	}
	\caption{Order of Annihilations}\label{alg:order}
\end{algorithm}
Since this order of annihilation depends only on the matrix dimension, it does not change throughout the outer
loop of the trace minimization algorithms except that for the Rayleigh-Ritz procedure in TraceMin-Davidson algorithm.
Hence, the order can be computed before the outer loop and reused throughout the iterations. For Tracemin-Davidson
algorithm, new order needs to be recomputed since the dimension increases over iterations.

The two Jacobi methods differ from each other in some ways. For the 1-sided Jacobi method, it finds an orthogonal
matrix $U$ such that $A U = Q$ is a matrix of orthogonal columns and $Q$ can be written as $V \Sigma$ with
$V^T V = I$; whereas for the 2-sided Jacobi method, it finds an orthogonal matrix $U$ such that $UAU^T = D$ and
$D$ is diagonal. Plane rotations only apply on columns for 1-sided Jacobi method but apply on both columns and rows
for 2-sided Jacobi method. The angle of plane rotation depends on the 2-norms of the columns and their inner
product for 1-sided Jacobi method, and the values of the $2 \times 2$ principal submatrix for 2-sided Jacobi method.
Finally, for 1-sided Jacobi method, the absolute values of the eigenvalues are the 2-norms of the columns of $Q$ and
the eigenvectors are found by scaling the columns by the inverse of the eigenvalues. For 2-sided Jacobi method, the
resulting diagonal matrix $D$ contains the eigenvalues and the columns of matrix $U^T$ which is the product of all
the plane rotations are the eigenvectors. Since the 1-sided Jacobi method naturally computes the absolute values of
the eigenvalues, it is more suitable to be used on the $B$-orthonormalization procedure since $B$ is symmetric
positive definite and the eigenvalues of $V^T B V$ are always positive. To find negative eigenvalues by the 1-sided
Jacobi method, one needs to calculate the Rayleigh quotient $v^T A v / \|v\|_2$. During the experiments, we found
that the 2-sided Jacobi method is more numerically stable when calculating negative eigenvalues. Therefore, we
apply the 2-sided Jacobi method for the Rayleigh-Ritz procedure althogh the 1-sided Jacobi method is computationally
less expensive.

\subsection{Modified Conjugate Gradient Method}
We implemented the {\tt CG} linear solver for the linear system (eq.9) when A is s.p.d.  Since the linear solver would take up a large chunk of the time. We did some modifications to help us accelerate the progress. such as\\1. Avoid calculating the matrix inverse;\\2. Make it parallel for different linear equations;\\3. Do a convergence test to stop the iterations earlier.


In order to avoid calculating the matrix inverses of the projection matrix $P=I-BY(Y^TB^2Y)^{-1}Y^TB$, we can find a orthogonal base $Q$ of $BY$ by QR-factorization. Let's note $Q=[Q_1, Q_2]$. It is easy to prove that $P=I-Q_1Q_1^T$. Also we should keep in mind that the $Q1Q1^T$ should not be explicitly calculated out since there would be a lot of fill-ins. Then we can calculate the $P\mathcal{A}$ by $\mathcal{A}-Q_1(Q_1^T\mathcal{A})$ from the right to the left in future.  


Also since the Mat-Vec operation (line3 of Algorithm 5) takes up a lot of time. We want to parallel the conjugate gradient iterative method by utilize the fact that our linear system contains multiple equations. 

There are two designs, one is to $modify$ CG to support the multiple-equations in parallel another is just $call$ CG algorithms in parallel.

The first one want us to implement own  matrix operations "$\times$" in (line 4, 9),$\otimes$ (in line 5,6 and 10) and $/$ in (line 4, 9) of the algorithm 5, as showed during the presentation. For The other designs, we just call the CG for each linear equations in parallel, we can use OpenMP to do calculate each columns separately.

The advantage of the first one is that the Mat-Vec operation of line3 of algorithm5 is BLAST3 instead of BLAST2. But in practice, there would be a lag because of different convergence speed. Since the OpenMP neither provides us a “mask” to avoid creating the unnecessary threads (like that on CUDA of GPU platform), nor allows the thread to break individually. Then the threads of the converged linear equations had to wait until the slowest one to quit. 
 
In order to diminish the above lags, I maintained a global array of flags representing whether an equation had converged already. And each OpenMP thread will check the relative flags before it performance its tasks.

According to the lecture, we can accelerate the CG by choosing a smaller iteration number m. Such that the approximation result would accurate enough. according to the Prof. Sameh's work, \ref{alg:basic} we can prove that, whether $\alpha r_1^Tr_1$ smaller than $\sigma^2b^Tb$ where $b$ is the initial residual and $\sigma$ is the approximation of the eigenvalue $\lambda_i$ divided by the largest eigenvalue $\lambda_{s+1} $.  
\begin{algorithm}[h]
	\SetArgSty{}
	\SetKwProg{Proc}{}{ :}{}
	\KwIn{$A$ a symmetric positive definite matrix, \newline$R$ the right hand side Matrix
	}
	\KwOut{$X$ is the solution of $(I-Q_1Q_1^T)AX = R$}
	\Kw{$wocao$}
	{$r_0 = p = R$\;}
	\For{$k = 1 \rightarrow$ max\_iter}{
	   {$\mathcal{P} = A\circ p - Q_1(Q_1^T(A\circ p))$\;}
	   {$\alpha = r_0^T\times r_0\ /\ (p^T\times \mathcal{P})$\;}
	   {$X   = X+\alpha \otimes p$\;}
	   {$r_1 = r_0 -\alpha\otimes \mathcal{P}$\;}
	   \If{test converge or $r_1$ is sufficient small} 
	   {
	    {$break$\;}
	    }
	    { $\beta = r_1^T\times r_1 \ / \ (r_0^T\times r_0) $\;}
	    { $p = r_1 + \beta \otimes p$\;}
	    
	}
	\caption{Modified Conjugate Gradient Algorithm}
	\label{alg:modifiedcg}
\end{algorithm}


\subsection{Minimum Residual Method}
We implemented the {\tt MinRes} linear solver as the class note described. The {\tt MinRes} linear solver works in the case when $A$ is not s.p.d, which is required in the multisection algorithm.  The flowchart of the algorithm is given in Algorithm~\ref{alg:minres}. Out implementation is a directly transcript of the describe algorithm, except that we solve each column in $N_k$ independently to get each column of $\Delta_k$. And we replace the matrix-vector multiplication $Ax$ in the {\tt Arnoldi process} into $\left(I-Q_1 Q_1^T\right)Ax$, because the system we are solving is $\left(I-Q_1 Q_1^T\right)A \Delta_k = N_k$. In each iteration, only one matrix vector multiplication is needed. And we implement the algorithm using {\tt Intel MKL}.

\begin{algorithm}[!ht]
	\SetArgSty{}
	\SetKwProg{Proc}{}{ :}{}
	\KwIn{$A$ a symmetric matrix, \newline
		 $rhs$ the right hand side vector
	}
	\KwOut{Vector $x$ is the solution of $Ax = rhs$}
	{$x_0 = 0\;$}
	{$v_1 = rhs$\;}
	{$\beta_1 = \| v_1 \|_2$\;}
	{$v_1 = v_1 / \beta_1$\;}
	\For{$k = 1 \rightarrow$ mat\_iter, or until convergence}{
	   \Proc{{\tt Arnoldi process}}{
	       $v_{k+1} = A v_{k}$\;
	       $\alpha_k = <v_k, v_{k+1}>$\;
	       $v_{k+1}  = v_{k+1} - \alpha_k v_k$\;
	       $\beta_{k+1} = \| v_{k+1}\|_2$\;
	       $v_{k+1} = v_{k+1} / \beta_{k+1}$\;
	   }
	   \Proc{Use $c_{k-2}, s_{k-2}, c_{k-1}, s_{k-1}$ to rotate $[0, \beta_{k}, \alpha_k, \beta_{k+1}]$} {
	       $[\epsilon_k, \delta_k, \dot{r}_k, \beta_{k+1}] = [0, \beta_{k}, \alpha_k, \beta_{k+1}]$ \newline $\begin{bmatrix} c_{k-2} & s_{k-2} & 0 & 0 \\ -s_{k-2} & c_{k-2} & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 0 & 0 & 0 \\ 0 & c_{k-1} & s_{k-1} & 0 \\ 0 & -s_{k-1} & c_{k-1} & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$
	   }
	   \Proc{Get $c_k, s_k$ to rotate $[\epsilon_k, \delta_k, \dot{r}_k, \beta_{k+1}]$ to $[\epsilon_k, \delta_k, r_k, 0]$ } {
	      $[\epsilon_k, \delta_k, r_k, 0] = [\epsilon_k, \delta_k, \dot{r}_k, \beta_{k+1}] \begin{bmatrix} 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 \\ 0 & 0 & c_k & s_k \\ 0 & 0 & -s_k & c_k\\ \end{bmatrix}$
	   }
	   {Solve $m_k$ from $\epsilon_k m_{k-2} + \delta_k m_{k-1} + r_k m_k = v_k$\;}
	   {$\tau_k = s_1 \cdots s_{k-1} c_k$\;}
	   {Update $x_k = x_{k-1} + \beta_1 \tau_k v_k$\;}
	   {Check convergence $r_k = A x_k - rhs$\;}
	}
	\caption{Minimum Residual Algorithm}
	\label{alg:minres}
\end{algorithm}

\subsection{Multisectioning}
When we want to compute all eigenvalues in the interval $[a, b]$. We can first perform $LDL^T$ factorization on $A-aB$ and $A-bB$ respectively. Then by counting the number of negative elements along the diagonal of $D$, we can know how many eigenvalues are smaller than $a$ and $b$ respectively in the problem $A x = \lambda B x$. Then we can know how many eigenvalues are in the interval $[a,b]$, assume it's $p$. Then we can shift the origin to $(a+b)/2$ and apply the TraceMin algorithm to solve $\left[A - (a+b)/2 B\right] x = \lambda B x$ for the first $p$ smallest absolute eigenvalues. 

To accelerate the procedure, we would like to further divide the interval $[a,b]$ into smaller subsections, and we also want each subsection to contain roughly same amount of eigenvalues. Then we can run multiple TraeMin algorithm on each subsection on different processes. So the problem is reduced to how to find the subsections. We call this problem as multisection and we developed a heuristic approach to solve the problem.

Assume we have $p$ processors, and we fix a parameter $s$ as in each round, each processor will run $s$ times of $LDL^T$ factorization. In the first round of the algorithm, the entire interval $[a, b]$ is divided in to $p \times s - 1$ subintervals, so there are $p \times s$ endpoints. Then after each processor performs $s$ times $LDL^T$ factorization, we can know the total number of eigenvalues in the entire interval $[a,b]$, the expected average $e_a$ after multisection and the number of eigenvalues in each of the $p \times s-1$ subinterval.

Then we check for all subintervals, if the number of eigenvalue in that interval is greater than $e_a$ times a tolerance, $10\%$ in our implementation. If it does, we further divide this subinterval in to $p \times s+1$ subsubintervals, and let each processor run $s$ times $LDL^T$ factorizations to calculate the number of eigenvalues on each subsubintervals. We repeat this process until the number of eigenvalue in all finest intervals are smaller than $e_a \times 10\%$. 

After that we begin with the left most endpoint $a$, keep combining subintervals until the number of eigenvalues are close to the expected average $e_a$. Then we say this is a subsection in the multisection. We keep this procedure until the entire interval $[a,b]$ is divide into $p$ subsections. And we also know the number of eigenvalues in each subsection. Then we can run $p$ TraceMin algorithms on $p$ nodes in parallel to evaluate all eigenvalues in interval $[a,b]$.

In the implementation, we set $s = 4$ and tolerance as $10\%$. The rational behind the tolerance is that we can guarantee that the difference between the number of eigenvalues in each subsection and the expected average will be no more than $ea \times 5\%$. And we use {\tt PARDISO} to compute the $LDL^T$ factorization.



